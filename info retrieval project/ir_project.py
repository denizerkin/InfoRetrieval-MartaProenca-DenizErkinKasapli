# -*- coding: utf-8 -*-
"""IR_SVM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jAbcMf03723HcZMaX53lUTwoX4enbVTW

# NAIVE BAYES CLASSIFIER
"""

from google.colab import files
uploaded = files.upload()

#RANDOM SPLITTER
import numpy as np
from sklearn.model_selection import train_test_split
import math
from csv import reader
labels = []
ids=[]
tweets=[]
number_of_docs = 0

with open('tweet_emotions.csv','r') as read_obj:
  csv_reader=reader(read_obj)
  for row in csv_reader:
    ids.append(row[0])
    labels.append(row[1])
    tweets.append(row[2])

tweets_train, tweets_test, labels_train, labels_test = train_test_split(tweets, labels, test_size=0.2, random_state=33)

from sklearn.feature_extraction.text import CountVectorizer
count_vect_n_random = CountVectorizer()
X_train_counts_n_random = count_vect_n_random.fit_transform(tweets_train)

from sklearn.feature_extraction.text import TfidfTransformer
tfidf_transformer_n_random = TfidfTransformer()
X_train_tfidf_n_random = tfidf_transformer_n_random.fit_transform(X_train_counts_n_random)

from sklearn.naive_bayes import MultinomialNB #naived bayes 
clf_n_random = MultinomialNB().fit(X_train_tfidf_n_random, labels_train)

X_new_counts_n_random = count_vect_n_random.transform(tweets_test)
print(labels_test)
print(len(labels_test)) #groundtruth
X_new_tfidf_n_random = tfidf_transformer_n_random.transform(X_new_counts_n_random)
predicted_n_random = clf_n_random.predict(X_new_tfidf_n_random)

#calculate precision
true_positives=0
false_positives=0
true_negatives=0
false_negatives=0


for index in range(len(labels_test)): #arrays have the same size, dont need 2 loops
  if labels_test[index]==predicted_n_random[index]:
    true_positives+=1
   
  else:
    false_positives+=1
 
precision_n_random=true_positives/(true_positives+false_positives)
print(precision_n_random)

#MANUAL SPLITTER
import math
from csv import reader
labels = []
ids=[]
tweets=[]
training_tweets=[]
test_tweets=[]
training_labels=[]
test_labels=[]
number_of_docs = 0

with open('tweet_emotions.csv','r') as read_obj:
  csv_reader=reader(read_obj)
  for row in csv_reader:
    ids.append(row[0])
    labels.append(row[1])
    tweets.append(row[2])

number_of_docs=len(tweets)
split=math.floor(number_of_docs*0.8)
for doc in range(number_of_docs):
  if doc < split:
    training_tweets.append(tweets[doc])
  else:
   test_tweets.append(tweets[doc])

number_of_labels=len(labels)
splitl=math.floor(number_of_labels*0.8)
for doc in range(number_of_labels):
  if doc < splitl:
    training_labels.append(labels[doc])
  else:
   test_labels.append(labels[doc])

count_vect_n_manual = CountVectorizer()
X_train_counts_n_manual = count_vect_n_manual.fit_transform(training_tweets)

tfidf_transformer_n_manual = TfidfTransformer()
X_train_tfidf_n_manual = tfidf_transformer_n_manual.fit_transform(X_train_counts_n_manual)

clf_n_manual = MultinomialNB().fit(X_train_tfidf_n_manual, training_labels)

X_new_counts_n_manual = count_vect_n_manual.transform(test_tweets)
print(test_labels)
print(len(test_labels)) #groundtruth
X_new_tfidf_n_manual = tfidf_transformer_n_manual.transform(X_new_counts_n_manual)

predicted_n_manual = clf_n_manual.predict(X_new_tfidf_n_manual)

#calculate precision
true_positives=0
false_positives=0
true_negatives=0
false_negatives=0


for index in range(len(test_labels)): #arrays have the same size, dont need 2 loops
  if test_labels[index]==predicted_n_manual[index]:
    true_positives+=1
   
  else:
    false_positives+=1
 
precision_n_manual=true_positives/(true_positives+false_positives)
print(precision_n_manual) #naive bayes classifier so low precision

#COMPARING SPLITTING METHODS
import numpy as np
import pandas as pd
from pandas import Series, DataFrame
import matplotlib.pyplot as plt

data = [precision_n_manual,precision_n_random]
plt.bar(['Manual Splitter','Random Splitter'], data)
plt.show()

#CONFUSION MATRIX FOR RANDOM SPLITTING
from sklearn.metrics import confusion_matrix
cm_naive = confusion_matrix(labels_test,predicted_n_random)

import matplotlib.pyplot as plt
import seaborn as sn
plt.figure(figsize=(10,7))
sn.heatmap(cm_naive, annot=True)
plt.xlabel("Predicted")
plt.ylabel("Truth")

"""# RANDOM FOREST CLASSIFIER"""

#RANDOM SPLITTER
import numpy as np
from sklearn.model_selection import train_test_split
import math
from csv import reader
labels = []
ids=[]
tweets=[]
number_of_docs = 0


with open('tweet_emotions.csv','r') as read_obj:
  csv_reader=reader(read_obj)
  for row in csv_reader:
    ids.append(row[0])
    labels.append(row[1])
    tweets.append(row[2])
  

labels.pop(0)
tweets.pop(0)

tweets_train, tweets_test, labels_train, labels_test = train_test_split(tweets, labels, test_size=0.2, random_state=33)

count_vect_r_random = CountVectorizer()
X_train_counts_r_random = count_vect_r_random.fit_transform(tweets_train) #lines-number of documents and distinct words in the columns

tfidf_transformer_r_random = TfidfTransformer()
X_train_tfidf_r_random = tfidf_transformer_r_random.fit_transform(X_train_counts_r_random)

from sklearn.ensemble import RandomForestClassifier
model_r_random = RandomForestClassifier()
model_r_random.fit(X_train_tfidf_r_random,labels_train)

X_new_counts_r_random = count_vect_r_random.transform(tweets_test)
X_new_tfidf_r_random = tfidf_transformer_r_random.transform(X_new_counts_r_random)

precision_r_random= model_r_random.score(X_new_tfidf_r_random,labels_test)
print(precision_r_random)

import math
from csv import reader
labels = []
ids=[]
tweets=[]
training_tweets=[]
test_tweets=[]
training_labels=[]
test_labels=[]
number_of_docs = 0

with open('tweet_emotions.csv','r') as read_obj:
  csv_reader=reader(read_obj)
  for row in csv_reader:
    ids.append(row[0])
    labels.append(row[1])
    tweets.append(row[2])

number_of_docs=len(tweets)
split=math.floor(number_of_docs*0.8)
for doc in range(number_of_docs):
  if doc < split:
    training_tweets.append(tweets[doc])
  else:
    test_tweets.append(tweets[doc])

number_of_labels=len(labels)
splitl=math.floor(number_of_labels*0.8)
for doc in range(number_of_labels):
  if doc < splitl:
    training_labels.append(labels[doc])
  else:
    test_labels.append(labels[doc])

count_vect_r_manual = CountVectorizer()
X_train_counts_r_manual = count_vect_r_manual.fit_transform(training_tweets)

tfidf_transformer_r_manual = TfidfTransformer()
X_train_tfidf_r_manual = tfidf_transformer_r_manual.fit_transform(X_train_counts_r_manual)

model_r_manual = RandomForestClassifier()
model_r_manual.fit(X_train_tfidf_r_manual,training_labels)

X_new_counts_r_manual = count_vect_r_manual.transform(test_tweets)
X_new_tfidf_r_manual = tfidf_transformer_r_manual.transform(X_new_counts_r_manual)

precision_r_manual = model_r_manual.score(X_new_tfidf_r_manual,test_labels)
print(precision_r_manual)

#COMPARE THE SPLITTING METHODS
data = [precision_r_manual,precision_r_random]
plt.bar(['Manual Splitter','Random Splitter'], data)
plt.show()

#USING RANDOM METHOD
predicted_labels_randomf = model_r_random.predict(X_new_tfidf_r_random)
cm_randomf = confusion_matrix(labels_test,predicted_labels_randomf)
plt.figure(figsize=(10,7))
sn.heatmap(cm_randomf, annot=True)
plt.xlabel("Predicted")
plt.ylabel("Truth")

"""# SVM MODEL

"""

from google.colab import files
uploaded = files.upload()

from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn import svm
from sklearn import metrics
from csv import reader

#RANDOM SPLITTER
import numpy as np
from sklearn.model_selection import train_test_split
import math
from csv import reader
labels = []
ids=[]
tweets=[]
number_of_docs = 0

with open('tweet_emotions.csv','r') as read_obj:
  csv_reader=reader(read_obj)
  for row in csv_reader:
    ids.append(row[0])
    labels.append(row[1])
    tweets.append(row[2])

tweets.pop(0)
labels.pop(0)

tweets_train, tweets_test, labels_train, labels_test = train_test_split(tweets, labels, test_size=0.2, random_state=33)

count_vect_svm_random = CountVectorizer()
X_train_counts_svm_random = count_vect_svm_random.fit_transform(tweets_train) #lines-number of documents and distinct words in the columns'''

tfidf_transformer_svm_random = TfidfTransformer()
X_train_tfidf_svm_random = tfidf_transformer_svm_random.fit_transform(X_train_counts_svm_random)

X_new_counts_svm_random = count_vect_svm_random.transform(tweets_test)
print(labels_test) #groundtruth
X_new_tfidf_svm_random = tfidf_transformer_svm_random.transform(X_new_counts_svm_random)

cls_random = svm.SVC(kernel="linear")
cls_random.fit(X_train_tfidf_svm_random,labels_train)

pred_svm_random = cls_random.predict(X_new_tfidf_svm_random)
accuracy_svm_random=metrics.accuracy_score(labels_test,pred_svm_random)
print(accuracy_svm_random)
print("accuracy: ", metrics.accuracy_score(labels_test,pred_svm_random))

#MANUAL SPLITTER
import math
from csv import reader
labels = []
ids=[]
tweets=[]
training_tweets=[]
test_tweets=[]
training_labels=[]
test_labels=[]
number_of_docs = 0

with open('tweet_emotions.csv','r') as read_obj:
  csv_reader=reader(read_obj)
  for row in csv_reader:
    ids.append(row[0])
    labels.append(row[1])
    tweets.append(row[2])

number_of_docs=len(tweets)
split=math.floor(number_of_docs*0.8)
for doc in range(number_of_docs):
  if doc < split:
    training_tweets.append(tweets[doc])
  else:
    test_tweets.append(tweets[doc])

number_of_labels=len(labels)
splitl=math.floor(number_of_labels*0.8)
for doc in range(number_of_labels):
  if doc < splitl:
    training_labels.append(labels[doc])
  else:
    test_labels.append(labels[doc])

count_vect_svm_manual = CountVectorizer()
X_train_counts_svm_manual = count_vect_svm_manual.fit_transform(training_tweets)

tfidf_transformer_svm_manual = TfidfTransformer()
X_train_tfidf_svm_manual = tfidf_transformer_svm_manual.fit_transform(X_train_counts_svm_manual)

X_new_counts_svm_manual = count_vect_svm_manual.transform(test_tweets)
print(test_labels) #groundtruth
X_new_tfidf_svm_manual = tfidf_transformer_svm_manual.transform(X_new_counts_svm_manual)

cls_svm_manual = svm.SVC(kernel="linear")
cls_svm_manual.fit(X_train_tfidf_svm_manual,training_labels)

pred_svm_manual = cls_svm_manual.predict(X_new_tfidf_svm_manual)
accuracy_svm_manual= metrics.accuracy_score(test_labels,pred_svm_manual)
print(accuracy_svm_manual)
print("accuracy: ", metrics.accuracy_score(test_labels,pred_svm_manual))

#BAR CHART TO COMPARE TWO SPLITTING METHODS
import numpy as np
import pandas as pd
from pandas import Series, DataFrame
import matplotlib.pyplot as plt

data = [accuracy_svm_manual,accuracy_svm_random]
plt.bar(['Manual Splitter','Random Splitter'], data)
plt.show()

#use different vectorization model

#first splitting the dataset
import numpy as np
from sklearn.model_selection import train_test_split
import math
from csv import reader
labels_diff = []
ids_diff=[]
tweets_diff=[]
number_of_docs = 0


with open('tweet_emotions.csv','r') as read_obj:
  csv_reader=reader(read_obj)
  for row in csv_reader:
    ids_diff.append(row[0])
    labels_diff.append(row[1])
    tweets_diff.append(row[2])
  

labels_diff.pop(0)
tweets_diff.pop(0)

#installing necessary things
!pip install spacy
!python -m spacy download en_core_web_lg


import spacy
import en_core_web_lg
nlp = en_core_web_lg.load()

#vectorizing function
def get_vec(x):
  doc = nlp(x)
  vec = doc.vector
  return vec

vec = [get_vec(x) for x in tweets_diff]

import pandas as pd
vec = pd.DataFrame(vec)
X_diff = vec.to_numpy()
X_diff = X_diff.reshape(-1,1)
X_diff = X_diff.reshape(-1, 300)

#randomly split the dataset
tweets_train_diff, tweets_test_diff, labels_train_diff, labels_test_diff = train_test_split(X_diff, labels_diff, test_size=0.2, random_state=33)


from sklearn import svm
from sklearn.svm import SVC

# with only two features "Happiness and Sadness"

import math
from csv import reader
labels_twofeatures = []
ids_twofeatures=[]
tweets_twofeatures=[]

number_of_docs = 0

with open('tweet_emotions.csv','r') as read_obj:
  csv_reader=reader(read_obj)
  for row in csv_reader:
    if row[1] == "happiness" or row[1] == "sadness":
      ids_twofeatures.append(row[0])
      labels_twofeatures.append(row[1])
      tweets_twofeatures.append(row[2])


tweets_train_twofeatures, tweets_test_twofeatures, labels_train_twofeatures, labels_test_twofeatures = train_test_split(tweets_twofeatures, labels_twofeatures, test_size=0.2, random_state=33)

count_vect_twofeatures = CountVectorizer()
X_train_counts_twofeatures = count_vect_twofeatures.fit_transform(tweets_train_twofeatures)

tfidf_transformer_twofeatures = TfidfTransformer()
X_train_tfidf_twofeatures = tfidf_transformer_twofeatures.fit_transform(X_train_counts_twofeatures)


cls_random_twofeatures = svm.SVC(kernel="linear")
cls_random_twofeatures.fit(X_train_tfidf_twofeatures,labels_train_twofeatures)

X_new_counts_svm_twofeatures = count_vect_twofeatures.transform(tweets_test_twofeatures)
X_new_tfidf_svm_twofeatures = tfidf_transformer_twofeatures.transform(X_new_counts_svm_twofeatures)

pred_svm_twofeatures  = cls_random_twofeatures.predict(X_new_tfidf_svm_twofeatures)
accuracy_svm_twofeatures =metrics.accuracy_score(labels_test_twofeatures,pred_svm_twofeatures)
print("accuracy: ", accuracy_svm_twofeatures)

#BAR CHART TO COMPARE Different number of features
import numpy as np
import pandas as pd
from pandas import Series, DataFrame
import matplotlib.pyplot as plt

data = [accuracy_svm_twofeatures,accuracy_svm_random]
plt.bar(['Two Features','13 Features'], data)
plt.show()

from sklearn import metrics
#training SVM model
cls_diff = svm.SVC(kernel="linear")
cls_diff.fit(tweets_train_diff,labels_train_diff)
#print accuracy
pred_diff = cls_diff.predict(tweets_test_diff)
ac = metrics.accuracy_score(labels_test_diff,pred_diff)
print("accuracy: ",ac)

#BAR CHART TO COMPARE TWO VECTORIZATION METHODS
import numpy as np
import pandas as pd
from pandas import Series, DataFrame
import matplotlib.pyplot as plt

data = [ac,accuracy_svm_random]
plt.bar(['Word2Vec with Spacy',' Count Vectorizer'], data)
plt.show()

#USING MANUAL SPLITTER 
from sklearn.metrics import confusion_matrix
cm_svm = confusion_matrix(test_labels,pred_svm_manual)

import matplotlib.pyplot as plt
import seaborn as sn
plt.figure(figsize=(10,7))
sn.heatmap(cm_svm, annot=True)
plt.xlabel("Predicted")
plt.ylabel("Truth")

"""# USE THE SCRIPTS IN THE MODEL

"""

from google.colab import files
uploaded = files.upload()

emotions=['empty','sadness','neutral','enthusiasm','worry','surprise','love','fun','hate','relief']

with open('onlytext.txt', 'r') as f:
    lines = [line.strip() for line in f]

m0lines = []
i = 0
while i<len(lines):
  m0lines.append(lines[i])
    
  i+=1
print(m0lines)

movieLines_counts = count_vect_svm_random.transform(m0lines)
movieLines_tfidf = tfidf_transformer_svm_random.transform(movieLines_counts)
predicted_movie = cls_random.predict(movieLines_tfidf)
print(predicted_movie)

emo_counter = [0]*13
indexCounter = 0
for i in emotions:
  for j in predicted_movie:
    if i == j:
      emo_counter[indexCounter] += 1
  indexCounter+=1

print(emo_counter)

import pandas as pd
from matplotlib import pyplot as plt

x = np.arange(13)
plt.bar(x, emo_counter,color='tomato')
plt.xticks(x, emotions, rotation="vertical");

emotions_counted = []

allFiles = ["m0u0BiancaLines.txt","m0u1BruceLines.txt","m0u2CameronLines.txt","m0u3ChastityLines.txt","m0u4JoeyLines.txt","m0u5KatLines.txt","m0u6MandellaLines.txt","m0u7MichaelLines.txt","m0u8MissPerkyLines.txt","m0u9PatrickLines.txt","m0u10SharonLines.txt","m0u11WalterLines.txt"]

characters = ["Bianca","Bruce","Cameron","Chastity","Joey","Kat","Mandella","Michael","MissPerky","Patrick","Sharon","Walter"]

m = 0
for i in allFiles:  
  with open(i, 'r') as f:
      lines = [line.strip() for line in f]

  characterLines = []
  j = 0
  while j<len(lines):
    characterLines.append(lines[j])
      
    j+=1
  print(characterLines)

  characterLines_counts = count_vect_svm_random.transform(characterLines)
  characterLines_tfidf = tfidf_transformer_svm_random.transform(characterLines_counts)
  predicted_character = cls_random.predict(characterLines_tfidf)
  print(predicted_character)

  emo_counter = [0]*13
  indexCounter = 0
  for i1 in emotions:
    for i2 in predicted_character:
      if i1 == i2:
        emo_counter[indexCounter] += 1
    indexCounter+=1

  print(emo_counter)
  emotions_counted.append(emo_counter)

x = np.arange(13)
plt.bar(x, emotions_counted[0],color='pink')
plt.xticks(x, emotions, rotation="vertical");
plt.title(characters[0])

y = np.arange(13)
plt.bar(y, emotions_counted[1],color='blue')
plt.xticks(y, emotions, rotation="vertical");
plt.title(characters[1])

z = np.arange(13)
plt.bar(z, emotions_counted[2],color='red')
plt.xticks(z, emotions, rotation="vertical");
plt.title(characters[2])

a = np.arange(13)
plt.bar(a, emotions_counted[3],color='magenta')
plt.xticks(a, emotions, rotation="vertical");
plt.title(characters[3])

b = np.arange(13)
plt.bar(b, emotions_counted[4],color='black')
plt.xticks(b, emotions, rotation="vertical");
plt.title(characters[4])

c = np.arange(13)
plt.bar(c, emotions_counted[5],color='gold')
plt.xticks(c, emotions, rotation="vertical");
plt.title(characters[5])

d = np.arange(13)
plt.bar(d, emotions_counted[6],color='purple')
plt.xticks(d, emotions, rotation="vertical");
plt.title(characters[6])

e = np.arange(13)
plt.bar(e, emotions_counted[7],color='brown')
plt.xticks(e, emotions, rotation="vertical");
plt.title(characters[7])

f = np.arange(13)
plt.bar(f, emotions_counted[8],color='coral')
plt.xticks(f, emotions, rotation="vertical");
plt.title(characters[8])

g = np.arange(13)
plt.bar(g, emotions_counted[9],color='grey')
plt.xticks(g, emotions, rotation="vertical");
plt.title(characters[9])

h = np.arange(13)
plt.bar(h, emotions_counted[10],color='orchid')
plt.xticks(h, emotions, rotation="vertical");
plt.title(characters[10])